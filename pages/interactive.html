<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Visualizations - From Calculus to AI</title>
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="../css/interactive.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="container header-content">
            <div class="logo">
                <a href="../index.html">Calculus to AI</a>
            </div>
            <nav>
                <ul>
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="mathematical_foundations.html">Mathematical Foundations</a></li>
                    <li><a href="neural_networks.html">Neural Networks</a></li>
                    <li><a href="training.html">Training</a></li>
                    <li><a href="code_examples.html">Code Examples</a></li>
                    <li><a href="teaching.html">Teaching Resources</a></li>
                    <li><a href="interactive.html">Interactive</a></li>
                </ul>
            </nav>
            <button class="mobile-menu-btn">☰</button>
        </div>
    </header>

    <main class="container">
        <div class="breadcrumb">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li>Interactive Visualizations</li>
            </ul>
        </div>

        <h1>Interactive Visualizations and Demonstrations</h1>
        
        <section>
            <h2>Explore Mathematical Concepts Through Interactive Visualizations</h2>
            <p>This page provides interactive visualizations and demonstrations that help build intuition about the mathematical concepts underlying deep learning. These tools can be valuable for both your own understanding and for use in the classroom.</p>
            
            <div class="tabs">
                <div class="tab-nav">
                    <div class="tab-nav-item" data-tab="tab-chain-rule">Chain Rule</div>
                    <div class="tab-nav-item" data-tab="tab-gradient-descent">Gradient Descent</div>
                    <div class="tab-nav-item" data-tab="tab-neural-network">Neural Network</div>
                    <div class="tab-nav-item" data-tab="tab-backpropagation">Backpropagation</div>
                </div>
                
                <div id="tab-chain-rule" class="tab-content">
                    <h3>The Chain Rule in Neural Networks</h3>
                    <p>The chain rule is a fundamental concept in calculus that allows us to compute derivatives of composite functions. In neural networks, we use the chain rule to compute how changes in the weights affect the output through multiple layers.</p>
                    
                    <div class="image-container">
                        <img src="../images/chain_rule_visualization.png" alt="Chain Rule Visualization">
                        <p class="caption">Visualization of the chain rule in a neural network</p>
                    </div>
                    
                    <p>The chain rule states that if \(y = g(f(x))\), then:</p>
                    <p>\[\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} \quad \text{where} \quad u = f(x)\]</p>
                    
                    <p>In a neural network with multiple layers, we apply the chain rule repeatedly to compute derivatives through the network. This is the mathematical foundation of the backpropagation algorithm.</p>
                    
                    <div class="interactive-demo">
                        <h3>Chain Rule Visualization Code</h3>
                        <p>The following Python code generates a visualization of the chain rule in action:</p>
                        <pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

# Define functions
def f(x):
    return x**2

def g(x):
    return np.sin(x)

def composite(x):
    return g(f(x))  # g(f(x)) = sin(x^2)

# Calculate derivatives
def df_dx(x):
    return 2*x

def dg_du(u):
    return np.cos(u)

def dcomposite_dx(x):
    return dg_du(f(x)) * df_dx(x)  # Chain rule: dg/du * df/dx

# Generate data points
x = np.linspace(-2, 2, 1000)
u = f(x)
y = composite(x)

# Create visualization
plt.figure(figsize=(12, 8))

# Plot f(x) = x^2
plt.subplot(2, 2, 1)
plt.plot(x, u, 'b-')
plt.title('f(x) = x²')
plt.grid(True)

# Plot g(u) = sin(u)
plt.subplot(2, 2, 2)
plt.plot(u, g(u), 'r-')
plt.title('g(u) = sin(u)')
plt.grid(True)

# Plot composite function g(f(x)) = sin(x²)
plt.subplot(2, 2, 3)
plt.plot(x, y, 'g-')
plt.title('g(f(x)) = sin(x²)')
plt.grid(True)

# Plot derivative of composite function
plt.subplot(2, 2, 4)
plt.plot(x, dcomposite_dx(x), 'purple')
plt.title('d/dx[g(f(x))] = cos(x²) · 2x')
plt.grid(True)

plt.tight_layout()
plt.savefig('chain_rule_visualization.png', dpi=300)
plt.show()</code></pre>
                    </div>
                </div>
                
                <div id="tab-gradient-descent" class="tab-content">
                    <h3>Gradient Descent Optimization</h3>
                    <p>Gradient descent is an optimization algorithm used to minimize the loss function in neural networks. It works by iteratively moving in the direction of steepest descent (negative gradient) to find the minimum of the function.</p>
                    
                    <div class="image-container">
                        <img src="../images/gradient_descent_3d.png" alt="Gradient Descent 3D">
                        <p class="caption">3D visualization of gradient descent on a loss surface</p>
                    </div>
                    
                    <p>The basic update rule for gradient descent is:</p>
                    <p>\[\theta = \theta - \alpha \nabla J(\theta)\]</p>
                    
                    <p>Where:</p>
                    <ul>
                        <li>\(\theta\) represents the parameters (weights and biases)</li>
                        <li>\(\alpha\) is the learning rate (step size)</li>
                        <li>\(\nabla J(\theta)\) is the gradient of the loss function with respect to the parameters</li>
                    </ul>
                    
                    <div class="image-container">
                        <img src="../images/gradient_descent_contour.png" alt="Gradient Descent Contour">
                        <p class="caption">Contour plot showing gradient descent paths with different learning rates</p>
                    </div>
                    
                    <div class="interactive-demo">
                        <h3>Gradient Descent Visualization Code</h3>
                        <p>The following Python code generates visualizations of gradient descent optimization:</p>
                        <pre><code class="python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from mpl_toolkits.mplot3d import Axes3D

# Define a loss function (a simple quadratic function)
def loss_function(w1, w2):
    return w1**2 + w2**2 + 2*w1*w2 + 2

# Define the gradient of the loss function
def gradient(w1, w2):
    dw1 = 2*w1 + 2*w2
    dw2 = 2*w2 + 2*w1
    return np.array([dw1, dw2])

# Implement gradient descent
def gradient_descent(start_w1, start_w2, learning_rate, num_iterations):
    w1, w2 = start_w1, start_w2
    path_w1, path_w2, path_z = [w1], [w2], [loss_function(w1, w2)]
    
    for i in range(num_iterations):
        grad = gradient(w1, w2)
        w1 = w1 - learning_rate * grad[0]
        w2 = w2 - learning_rate * grad[1]
        path_w1.append(w1)
        path_w2.append(w2)
        path_z.append(loss_function(w1, w2))
    
    return np.array(path_w1), np.array(path_w2), np.array(path_z)

# Create a 3D visualization of the loss function with gradient descent paths
fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')

# Create a meshgrid for the parameter space
w1 = np.linspace(-4, 4, 100)
w2 = np.linspace(-4, 4, 100)
W1, W2 = np.meshgrid(w1, w2)
Z = loss_function(W1, W2)

# Plot the loss surface
surf = ax.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.8, linewidth=0)

# Run gradient descent with different learning rates
start_w1, start_w2 = 3.5, 3.5
learning_rates = [0.01, 0.05, 0.1, 0.2]
num_iterations = 50

# Plot gradient descent paths with different learning rates
for lr in learning_rates:
    path_w1, path_w2, path_z = gradient_descent(start_w1, start_w2, lr, num_iterations)
    ax.plot(path_w1, path_w2, path_z, 'o-', linewidth=2, markersize=4, label=f'Learning Rate = {lr}')

# Add labels and title
ax.set_xlabel('Weight 1')
ax.set_ylabel('Weight 2')
ax.set_zlabel('Loss')
ax.set_title('Gradient Descent Optimization on Loss Surface', fontsize=16)

# Add a legend
ax.legend(loc='upper right')

plt.savefig('gradient_descent_3d.png', dpi=300)
plt.show()</code></pre>
                    </div>
                </div>
                
                <div id="tab-neural-network" class="tab-content">
                    <h3>Neural Network Architecture</h3>
                    <p>Neural networks consist of layers of interconnected neurons that process information. Understanding the architecture of neural networks is essential for grasping how they learn from data.</p>
                    
                    <div class="image-container">
                        <img src="../images/neural_network_architecture.png" alt="Neural Network Architecture">
                        <p class="caption">Architecture of a neural network with input, hidden, and output layers</p>
                    </div>
                    
                    <p>Each neuron computes a weighted sum of its inputs, adds a bias term, and then applies an activation function:</p>
                    <p>\[z = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b\]</p>
                    <p>\[a = \sigma(z)\]</p>
                    
                    <p>Where:</p>
                    <ul>
                        <li>\(x_1, x_2, \ldots, x_n\) are the inputs to the neuron</li>
                        <li>\(w_1, w_2, \ldots, w_n\) are the weights associated with each input</li>
                        <li>\(b\) is the bias term</li>
                        <li>\(z\) is the weighted sum (pre-activation)</li>
                        <li>\(\sigma\) is the activation function</li>
                        <li>\(a\) is the output of the neuron (post-activation)</li>
                    </ul>
                    
                    <div class="interactive-demo">
                        <h3>Neural Network Visualization Code</h3>
                        <p>The following Python code generates a visualization of a neural network architecture:</p>
                        <pre><code class="python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Circle, Rectangle, FancyArrowPatch

def draw_neural_network(ax, layer_sizes, layer_names=None):
    """
    Draw a neural network architecture diagram
    
    Parameters:
    - ax: matplotlib axis to draw on
    - layer_sizes: list of integers, number of neurons in each layer
    - layer_names: list of strings, names for each layer
    """
    if layer_names is None:
        layer_names = [f'Layer {i+1}' for i in range(len(layer_sizes))]
    
    # Colors
    input_color = '#4472C4'
    hidden_color = '#ED7D31'
    output_color = '#70AD47'
    
    # Spacing parameters
    vertical_spacing = 1.0
    horizontal_spacing = 3.0
    neuron_radius = 0.3
    
    # Calculate positions
    n_layers = len(layer_sizes)
    layer_top_anchors = {}
    
    # Draw layers
    for i, (size, name) in enumerate(zip(layer_sizes, layer_names)):
        color = input_color if i == 0 else output_color if i == n_layers - 1 else hidden_color
        
        # Calculate vertical positions
        layer_height = size * vertical_spacing
        top_anchor = layer_height / 2
        layer_top_anchors[i] = top_anchor
        
        # Draw layer label
        ax.text(i * horizontal_spacing, top_anchor + 1, name, 
                ha='center', va='center', fontsize=12, fontweight='bold')
        
        # Draw neurons
        for j in range(size):
            y_pos = top_anchor - j * vertical_spacing
            circle = Circle((i * horizontal_spacing, y_pos), neuron_radius, 
                           color=color, ec='black', zorder=4)
            ax.add_patch(circle)
            
            # Add text inside neuron
            if size <= 10:  # Only add text for small networks
                ax.text(i * horizontal_spacing, y_pos, str(j+1), 
                        ha='center', va='center', color='white', fontsize=8)
    
    # Draw connections between layers
    for i in range(n_layers - 1):
        for j in range(layer_sizes[i]):
            for k in range(layer_sizes[i + 1]):
                y_start = layer_top_anchors[i] - j * vertical_spacing
                y_end = layer_top_anchors[i + 1] - k * vertical_spacing
                
                arrow = FancyArrowPatch(
                    (i * horizontal_spacing + neuron_radius, y_start),
                    ((i + 1) * horizontal_spacing - neuron_radius, y_end),
                    connectionstyle='arc3,rad=0.1',
                    arrowstyle='-',
                    color='gray',
                    alpha=0.5,
                    linewidth=0.5,
                    zorder=1
                )
                ax.add_patch(arrow)
    
    # Set axis properties
    ax.set_xlim(-1, (n_layers - 1) * horizontal_spacing + 1)
    max_height = max(layer_sizes) * vertical_spacing
    ax.set_ylim(-max_height/2 - 1, max_height/2 + 2)
    ax.set_aspect('equal')
    ax.axis('off')

# Create figure
fig, ax = plt.subplots(figsize=(12, 8))

# Define network architecture
layer_sizes = [4, 6, 6, 3]
layer_names = ['Input Layer', 'Hidden Layer 1', 'Hidden Layer 2', 'Output Layer']

# Draw the network
draw_neural_network(ax, layer_sizes, layer_names)

# Add title
plt.title('Neural Network Architecture', fontsize=16)

plt.tight_layout()
plt.savefig('neural_network_architecture.png', dpi=300)
plt.show()</code></pre>
                    </div>
                </div>
                
                <div id="tab-backpropagation" class="tab-content">
                    <h3>Backpropagation Algorithm</h3>
                    <p>Backpropagation is the algorithm used to train neural networks. It works by propagating the error backward through the network, using the chain rule to compute how each weight contributes to the error.</p>
                    
                    <div class="image-container">
                        <img src="../images/backpropagation.png" alt="Backpropagation">
                        <p class="caption">Visualization of the backpropagation process</p>
                    </div>
                    
                    <p>The backpropagation algorithm consists of the following steps:</p>
                    <ol>
                        <li>Forward Pass: Compute the output of the network for a given input</li>
                        <li>Compute Loss: Calculate the error between the predicted output and the actual output</li>
                        <li>Backward Pass: Propagate the error backward through the network using the chain rule</li>
                        <li>Update Weights: Adjust the weights using gradient descent to minimize the loss</li>
                    </ol>
                    
                    <div class="image-container">
                        <img src="../images/backpropagation_steps.png" alt="Backpropagation Steps">
                        <p class="caption">Step-by-step process of backpropagation</p>
                    </div>
                    
                    <div class="interactive-demo">
                        <h3>Simple Neural Network Implementation</h3>
                        <p>The following Python code implements a simple neural network with backpropagation:</p>
                        <pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        """
        Initialize a simple neural network with one hidden layer
        
        Parameters:
        - input_size: number of input features
        - hidden_size: number of neurons in the hidden layer
        - output_size: number of output neurons
        - learning_rate: learning rate for gradient descent
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Initialize weights and biases with random values
        # Xavier/Glorot initialization for better gradient flow
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)
        self.b2 = np.zeros((1, output_size))
        
        # For tracking loss during training
        self.loss_history = []
    
    def sigmoid(self, x):
        """Sigmoid activation function: 1 / (1 + exp(-x))"""
        # Clip x to avoid overflow
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        """Derivative of sigmoid: sigmoid(x) * (1 - sigmoid(x))"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def forward(self, X):
        """
        Forward propagation through the network
        
        Parameters:
        - X: input data of shape (batch_size, input_size)
        
        Returns:
        - Tuple containing:
          - y_pred: output predictions
          - cache: cached values for backpropagation
        """
        # First layer: input to hidden
        # z1 = X·W1 + b1 (matrix multiplication)
        z1 = np.dot(X, self.W1) + self.b1
        
        # Apply activation function
        a1 = self.sigmoid(z1)
        
        # Second layer: hidden to output
        # z2 = a1·W2 + b2 (matrix multiplication)
        z2 = np.dot(a1, self.W2) + self.b2
        
        # Apply sigmoid activation for output layer
        y_pred = self.sigmoid(z2)
        
        # Cache values for backpropagation
        cache = {
            'X': X,
            'z1': z1,
            'a1': a1,
            'z2': z2,
            'y_pred': y_pred
        }
        
        return y_pred, cache
    
    def compute_loss(self, y_pred, y_true):
        """
        Compute binary cross-entropy loss
        
        Parameters:
        - y_pred: predicted values
        - y_true: true values
        
        Returns:
        - loss: scalar loss value
        """
        # Binary cross-entropy loss
        # L = -1/m * Σ[y·log(ŷ) + (1-y)·log(1-ŷ)]
        m = y_true.shape[0]
        
        # Add small epsilon to avoid log(0)
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        
        loss = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        return loss
    
    def backward(self, y_pred, y_true, cache):
        """
        Backward propagation to compute gradients
        
        Parameters:
        - y_pred: predicted values
        - y_true: true values
        - cache: cached values from forward propagation
        
        Returns:
        - gradients: dictionary containing gradients for W1, b1, W2, b2
        """
        m = y_true.shape[0]
        
        # Get cached values
        X = cache['X']
        z1 = cache['z1']
        a1 = cache['a1']
        
        # Output layer gradient
        # dL/dz2 = (y_pred - y_true) * sigmoid_derivative(z2)
        # For binary cross-entropy with sigmoid, this simplifies to (y_pred - y_true)
        dz2 = y_pred - y_true
        
        # Gradient for W2: dL/dW2 = a1^T · dz2
        dW2 = 1/m * np.dot(a1.T, dz2)
        
        # Gradient for b2: dL/db2 = sum(dz2)
        db2 = 1/m * np.sum(dz2, axis=0, keepdims=True)
        
        # Hidden layer gradient using chain rule
        # dL/da1 = dz2 · W2^T
        da1 = np.dot(dz2, self.W2.T)
        
        # dL/dz1 = dL/da1 * sigmoid_derivative(z1)
        dz1 = da1 * self.sigmoid_derivative(z1)
        
        # Gradient for W1: dL/dW1 = X^T · dz1
        dW1 = 1/m * np.dot(X.T, dz1)
        
        # Gradient for b1: dL/db1 = sum(dz1)
        db1 = 1/m * np.sum(dz1, axis=0, keepdims=True)
        
        # Return gradients
        gradients = {
            'dW1': dW1,
            'db1': db1,
            'dW2': dW2,
            'db2': db2
        }
        
        return gradients
    
    def update_parameters(self, gradients):
        """
        Update parameters using gradient descent
        
        Parameters:
        - gradients: dictionary containing gradients for W1, b1, W2, b2
        """
        # Update weights and biases
        # W = W - learning_rate * dW
        self.W1 -= self.learning_rate * gradients['dW1']
        self.b1 -= self.learning_rate * gradients['db1']
        self.W2 -= self.learning_rate * gradients['dW2']
        self.b2 -= self.learning_rate * gradients['db2']
    
    def train(self, X, y, epochs=1000, print_every=100):
        """
        Train the neural network
        
        Parameters:
        - X: input data
        - y: target values
        - epochs: number of training iterations
        - print_every: print loss every n epochs
        
        Returns:
        - loss_history: list of loss values during training
        """
        self.loss_history = []
        
        for i in range(epochs):
            # Forward propagation
            y_pred, cache = self.forward(X)
            
            # Compute loss
            loss = self.compute_loss(y_pred, y)
            self.loss_history.append(loss)
            
            # Backward propagation
            gradients = self.backward(y_pred, y, cache)
            
            # Update parameters
            self.update_parameters(gradients)
            
            # Print loss every n epochs
            if i % print_every == 0:
                print(f"Epoch {i}, Loss: {loss:.4f}")
        
        return self.loss_history
    
    def predict(self, X):
        """
        Make predictions
        
        Parameters:
        - X: input data
        
        Returns:
        - predictions: binary predictions (0 or 1)
        """
        # Forward propagation
        y_pred, _ = self.forward(X)
        
        # Convert probabilities to binary predictions
        predictions = (y_pred > 0.5).astype(int)
        
        return predictions</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>Try It Yourself</h2>
            <p>The code examples provided on this page can be downloaded and run on your own computer. They provide a hands-on way to explore the mathematical concepts behind deep learning.</p>
            
            <div class="accordion">
                <div class="accordion-item">
                    <div class="accordion-header">
                        <h3>Setting Up Your Environment</h3>
                    </div>
                    <div class="accordion-content">
                        <p>To run these examples, you'll need Python with the following libraries installed:</p>
                        <pre><code class="bash">pip install numpy matplotlib scikit-learn</code></pre>
                        <p>These libraries provide the mathematical and visualization tools needed to implement and visualize neural networks and their training processes.</p>
                    </div>
                </div>
                
                <div class="accordion-item">
                    <div class="accordion-header">
                        <h3>Modifying the Examples</h3>
                    </div>
                    <div class="accordion-content">
                        <p>Here are some suggestions for modifying the examples to deepen your understanding:</p>
                        <ul>
                            <li>Try different activation functions (ReLU, tanh) and observe how they affect training</li>
                            <li>Experiment with different learning rates and observe convergence behavior</li>
                            <li>Add more layers to the neural network and see how it affects performance</li>
                            <li>Apply the neural network to different datasets and problems</li>
                            <li>Visualize the decision boundaries learned by the network</li>
                        </ul>
                        <p>By experimenting with these parameters, you can build intuition about how neural networks learn and how the mathematical concepts influence their behavior.</p>
                    </div>
                </div>
                
                <div class="accordion-item">
                    <div class="accordion-header">
                        <h3>Classroom Applications</h3>
                    </div>
                    <div class="accordion-content">
                        <p>These interactive visualizations can be valuable teaching tools in the classroom. Here are some ways to use them:</p>
                        <ul>
                            <li>Demonstrate the chain rule in action when teaching calculus</li>
                            <li>Show how gradient descent optimizes functions when teaching optimization</li>
                            <li>Illustrate how neural networks learn from data when teaching machine learning</li>
                            <li>Assign projects where students modify the code to explore different aspects of neural networks</li>
                            <li>Use the visualizations to motivate the study of calculus by showing its applications in AI</li>
                        </ul>
                        <p>These applications can help make abstract mathematical concepts more concrete and relevant for students.</p>
                    </div>
                </div>
            </div>
        </section>

        <div class="continue-exploring">
            <h3>Continue Exploring</h3>
            <ul>
                <li><a href="mathematical_foundations.html">Mathematical Foundations of Deep Learning</a></li>
                <li><a href="neural_networks.html">Neural Network Architecture and Forward Propagation</a></li>
                <li><a href="training.html">Training Neural Networks: Backpropagation and Gradient Descent</a></li>
                <li><a href="code_examples.html">Code Examples and Implementations</a></li>
                <li><a href="teaching.html">Teaching Resources and Classroom Applications</a></li>
            </ul>
        </div>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-column">
                    <h3>Navigation</h3>
                    <ul>
                        <li><a href="../index.html">Home</a></li>
                        <li><a href="mathematical_foundations.html">Mathematical Foundations</a></li>
                        <li><a href="neural_networks.html">Neural Networks</a></li>
                        <li><a href="training.html">Training</a></li>
                        <li><a href="code_examples.html">Code Examples</a></li>
                        <li><a href="teaching.html">Teaching Resources</a></li>
                        <li><a href="interactive.html">Interactive</a></li>
                    </ul>
                </div>
                <div class="footer-column">
                    <h3>Resources</h3>
                    <ul>
                        <li><a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning Book</a></li>
                        <li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" target="_blank">3Blue1Brown Neural Networks</a></li>
                        <li><a href="http://cs231n.stanford.edu/" target="_blank">Stanford CS231n</a></li>
                        <li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank">Neural Networks and Deep Learning</a></li>
                    </ul>
                </div>
                <div class="footer-column">
                    <h3>About</h3>
                    <ul>
                        <li><a href="#">About This Workshop</a></li>
                        <li><a href="#">Contact</a></li>
                    </ul>
                </div>
            </div>
            <div class="copyright">
                <p>&copy; 2025 From Calculus to AI Workshop. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
    <script src="../js/interactive.js"></script>
</body>
</html>
